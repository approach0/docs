## Usage
The commands generated from building are distributed in the `run` directories of most modules.
And a lot of commands are named `test-*.out` (such as `blob-index/run/test-blob-index.out`), they are for testing purpose.
Other commands are utility or exporting commands to be used by search engine users (e.g. `searchd/run/searchd.out`).

Here we only list a few commands that are considered important
commands and accomplish certain useful functionality.

In general, you can issue `command -h` in most important commands to see its command line options and usage description.

### Tex parser
Run our TeX parser to see the corresponding operator tree of a math expression. And often this command is used to investigate a TeX grammar parsing error in the indexing process described later.

Below is an example of parsing \\(\dfrac a b + c\\).
```
$ ./tex-parser/run/test-tex-parser.out
edit: \frac a b +c
return code:0
Operator tree:
     └──(plus) 2 son(s), token=ADD, path_id=2, ge_hash=1f17, fr_hash=c301.
           │──(frac) 2 son(s), token=FRAC, path_id=1, ge_hash=a7f3, fr_hash=7203.
           │     │──#1[`a'] token=VAR, path_id=1, ge_hash=c401, fr_hash=7303.
           │     └──#2[`b'] token=VAR, path_id=2, ge_hash=c501, fr_hash=7403.
           └──[`c'] token=VAR, path_id=3, ge_hash=c601, fr_hash=8903.

Subpaths (leaf-root paths/total subpaths = 3/4):
* VAR(0)/rank1(1)/FRAC(2)/ADD(2)/[path_id=1: type=normal, leaf symbol=`a', fr_hash=7303]
* FRAC(2)/ADD(2)/[path_id=1: type=gener, ge_hash=a7f3, fr_hash=7203]
* VAR(0)/rank2(1)/FRAC(2)/ADD(2)/[path_id=2: type=normal, leaf symbol=`b', fr_hash=7403]
* VAR(0)/ADD(2)/[path_id=3: type=normal, leaf symbol=`c', fr_hash=8903]
```
Type `\` followed by `Tab` to auto-complete some frequently used TeX commands

### Crawler
A Python script crawler (`demo/crawler/crawler-math.stackexchange.com.py`) is included specifically for crawling *math stackexchange*.
Users are asked to write their own crawlers if they are trying to crawl data from other websites.

Install BeautifulSoup4 used by demo crawler.
```
$ apt-get install python3-pip
$ pip3 install BeautifulSoup4
```

Debian users may also need to install pycurl:
```
$ apt-get install python3-pycurl
```

To crawl *math stackexchange* from page 1 to 3:
```
$ cd $PROJECT/demo/crawler
$ ./crawler-math.stackexchange.com.py --begin-page 1 --end-page 3
```
Crawler will output all harvest files (in JSON) to `./tmp` directory which is a conventional directory name for output and will be deleted if you issue `make clean`.

You can press Ctrl-C to stop crawler in the middle of crawling process.

The output of crawler for each post will have two files, one is `*.json` corpus file (for now it contains URL and plain text of the post extracted by crawler), another is `*.html` file, which is for previewing this post corpus. (to preview it, connect to Internet and open it with your browser)

As an option, you can skip the time-consuming crawling
process by directly
[downloading a small size corpus](/download/math-corpus-small.tar.bz2) (around 7 MB) to play around later using indexer.
This small corpus contains 1000 pages we previously
crawled from *math stackexchange*.

Another crawler script `crawler-artofproblemsolving.com.py` is available for crawling [artofproblemsolving.com](https://artofproblemsolving.com). Shout out to [@TheSil](https://github.com/TheSil) for contributing that! More crawler scripts are coming out, see our plan [here](TODO.html#consider-additional-indexing-sources).

### Indexer
After corpus is generated by crawler.
Indexer is used to index a single corpus file or a corpus/collection directory recursively.

```
$ cd $PROJECT/indexer
$ ./run/indexer.out -p ./test-corpus 2> error.log
```

`test-corpus` is a test corpus manually written for specific
testing purpose.
To index the corpus you have just generated by invoking
crawler, issue:

```
$ ./run/indexer.out -p ../demo/crawler/tmp/ 2> error.log
```

Again, the output (resulting index) is generated under
`./tmp` directory except when you specify `-o` option to name
a output directory.

If you are using indexer to add new documents into existing
index in multiple runs, you need to ensure that the newly added
documents are not previously indexed. Otherwise duplicate document
may occur in search results. (Current indexer does not support index
document update)

If you are indexing a corpus with Chinese words, use `-d`
option to specify CppJieba dictionary path when calling
`indexer.out`. This will slow down indexing but it enables
searcher/searchd to search Chinese terms later (also have to
to specify `-d` in searcher/searchd).

Note it is required to have typically at least 1 GB of memory
for our indexer to successfully run through a non-trivial size
of corpus without being killed by the OS.

### Single query searcher
To test and run a query on the index you have just created,
run a *single-query searcher* that takes your query, searches for
relevant documents and exits immediately.

There are three single-query search programs available:

* A transitional full-text searcher which only searches
**terms** (i.e. regular text without math-expressions), 
located at `search/run/test-term-search.out`.
* A math-only searcher which only searches math expressions,
located at `search/run/test-math-expr-search.out`
* A mixed-query searcher which handles both math-expression
and term queries, located at `search/run/test-search.out`

Given mixed-query searcher as an example, to run mixed-query searcher
with a test query "function" and TeX "\\(f(x) = x^2 + 1\\)" on index
`../indexer/tmp`, issue:

```sh
$ cd $PROJECT/search
$ ./run/test-search.out -i ../indexer/tmp -t 'function' -m 'f(x) = x^2 + 1'
```

This searcher returns the first "page" of top-K relevant search
results (relevant keywords are highlighted in console). You
can use `-p` option to specify another page to be returned.

Please refer to command `-h` output for how to use the other
two searcher commands.

### Search daemon
On the top of our search engine modules is search daemon
program `searchd`, located at `searchd/run/searchd.out`.
It runs as a HTTP daemon that deals with every query (in JSON)
sent to it and return search results (in JSON too), never
exits unless you hit Ctrl-C.

The whole point of daemonize search service is for efficiency.
This is very easy to understand because there are obviously
large overheads in loading dictionary, and setting up
necessary search environment. Among those, the most significant
thing is caching.

Indeed, our searchd can be specified to cache a portion of
index posting into memory (currently only term index, future
will support math index too).
You can specify the maximum cache limit using `-c` option
followed by a number (in MB).
Current default cache limit is just 32MB.

To run searchd,
```
$ cd $PROJECT/searchd
$ ./run/searchd.out -i <index path> &
```

You can then test searchd by running *curl* scripts existing
in searchd directory:

```
$ ./scripts/test-query.sh ./tests/query-valid.json
```

To shutdown searchd, type command
```
$ kill -INT <pid>
```
